[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The materials provided in this repository are FREE to use.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement."
  },
  {
    "objectID": "module6/module6.html",
    "href": "module6/module6.html",
    "title": "Module 6",
    "section": "",
    "text": "Tools for bioinformatics\n\nChatGPT"
  },
  {
    "objectID": "data-flo/data-flo_doc.html",
    "href": "data-flo/data-flo_doc.html",
    "title": "Data-flo documentation",
    "section": "",
    "text": "https://docs.data-flo.io/introduction/readme"
  },
  {
    "objectID": "microreact/microreact_doc.html",
    "href": "microreact/microreact_doc.html",
    "title": "Microreact documentation",
    "section": "",
    "text": "https://docs.microreact.org/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "This page is a repository for training resources designed and developed for the short course on Bacterial Genomics Bioinformatics, Babes-Bolyai University, Cluj-Napoca, Romania, November 2024, by Monica Abrudan.\nThe materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\nFind out more about the author here"
  },
  {
    "objectID": "module2/module2.html",
    "href": "module2/module2.html",
    "title": "Module 2",
    "section": "",
    "text": "Follow the steps in Module 1 to connect to your remote virtual machine.\nThe next exercise is broadly based on the study https://journals.asm.org/doi/full/10.1128/msphere.00185-23 by Abrudan and Shamanna, 2023\n\n\n\nThe European Nucleotide Archive (ENA) provides a comprehensive record of the world’s nucleotide sequencing information, covering raw sequencing data, sequence assembly information and functional annotation.\nAccess to ENA data is provided through the browser, through search tools, through large scale file download and through the API."
  },
  {
    "objectID": "module2/module2.html#reads-qc",
    "href": "module2/module2.html#reads-qc",
    "title": "Module 2",
    "section": "Reads QC",
    "text": "Reads QC\nIn this part of the exercise, we will use a programme called FastQC.\nFastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\nThe main functions of FastQC are\n\nImport of data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your data\nExport of results to an HTML based permanent report\n\n\nRun FastQC programatically\nTo run non-interactively you simply have to specify a list of files to process on the command line. \nfastqc somefile.txt someotherfile.txt\nfastqc /home/ubuntu/data/*/*/*\n\n\nQC your fastq reads through the visual interface\nNavigate to /home/ubuntu/Software/FastQC and double click on the FastQC icon.\nIn the visual interface, open all files produced by FastQC and assess the results, eg ERR4635696_1_fastqc.html\nVisualize and interpret the FastQC results. Compare the QC results for Nanopore and Illumina sequence reads\nOptional: Unzip the files /home/ubuntu/Data/*/*/*_*_fastqc.zip and produce a script that extracts the information on reads lengths, GC contents etc; plot the results from all samples."
  },
  {
    "objectID": "module2/module2.html#speciation",
    "href": "module2/module2.html#speciation",
    "title": "Module 2",
    "section": "Speciation",
    "text": "Speciation\nBactinspector is a package to a) determine the most probable species based on sequence in fasta/fastq files using refseq and Mash (https://mash.readthedocs.io/en/latest/index.html) and b) determine the closest reference in refseq to a set of fasta/fastq files.\n\nGo to sample files /home/ubuntu/Data/ and run Bactinspector using the following command\n\nbactinspector closest_match -fq \"*_2.fastq.gz\"\n\nCheck the output *.tsv file\n\ntail -n +2 *.tsv| column -t | less -S\ncat *.tsv | tr \"\\t\" \"~\" | cut -d\"~\" -f2\nEg of a result:\nASM289538v1\nGCF_002895385\n\n\nHow do you interpret the results? Look up your results in ENA."
  },
  {
    "objectID": "module2/module2.html#genome-assembly",
    "href": "module2/module2.html#genome-assembly",
    "title": "Module 2",
    "section": "Genome assembly",
    "text": "Genome assembly\nThe Velvet assembler\nVelvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions.\n\nRun Velvet\n\nVisualise the assembles using ACT and Artemis\nACT is a Java application for displaying pairwise comparisons between two or more DNA sequences.\nACT can be used to identify and analyse regions of similarity and difference between genomes and to explore conservation of synteny, in the context of the entire sequences and their annotation. It can read complete EMBL, GENBANK and GFF entries or sequences in FASTA or raw format.\nTip: ACT and Artemis are installed here: /usr/share/miniconda/pkgs/artemis-18.2.0-hdfd78af_0/share/artemis-18.2.0-0"
  },
  {
    "objectID": "module2/module2.html#assembly-qc-with-quast",
    "href": "module2/module2.html#assembly-qc-with-quast",
    "title": "Module 2",
    "section": "Assembly QC with Quast",
    "text": "Assembly QC with Quast\nQUAST stands for QUality ASsessment Tool. It evaluates genome/metagenome assemblies by computing various metrics. The current QUAST toolkit includes the general QUAST tool for genome assemblies, MetaQUAST, the extension for metagenomic datasets, QUAST-LG, the extension for large genomes (e.g., mammalians), and Icarus, the interactive visualizer for these tools.\nThe QUAST package works both with and without reference genomes. However, it is much more informative if at least a close reference genome is provided along with the assemblies. The tool accepts multiple assemblies, thus is suitable for comparison."
  },
  {
    "objectID": "module3/module3_answers.html",
    "href": "module3/module3_answers.html",
    "title": "Bioinformatic methods for microbial genomics and transcriptomics, Cluj-Napoca, November 2024",
    "section": "",
    "text": "Run ARIBA on all samples\nExample script: create the following bash file: mkdir /home/ubuntu/Data/ARIBA_run/run_ariba.sh\nAdd the following lines of code:\nfor sample in `ls /home/ubuntu/Data/all_fastqs/*_1.fastq.gz | sed 's/\\_1.fastq.gz//'`\ndo\necho $sample\noutput=$(echo $sample | sed -E 's#.*/([^/]+)$#\\1#')\nif [ -d /home/ubuntu/Data/ARIBA_output/${output}.ariba ]; then\nrm -r /home/ubuntu/Data/ARIBA_output/${output}.ariba\nfi\nariba run /home/ubuntu/Data/ARIBA_dbs/out.resfinder.prepareref ${sample}_1.fastq.gz ${sample}_2.fastq.gz /home/ubuntu/Data/ARIBA_output/${output}.ariba\ndone"
  },
  {
    "objectID": "module4/module4.html",
    "href": "module4/module4.html",
    "title": "Module 4",
    "section": "",
    "text": "SARS-CoV-2 surveillance in Romania\nGenomic surveillance of pathogens aims to understand the emergence and dissemination of pathogens or their lineages of risk with the ultimate goal of implementing evidence-based interventions to protect public health. Epidemiological data from patients is collected by healthcare professionals. Data on species identification and any phenotypic or molecular characterization of the isolates is often generated by the microbiology laboratories linked to healthcare facilities and/or the reference laboratory. In an ideal scenario, the different sources of laboratory data are stored in a centralised surveillance system and database (such as WHONET). However, these systems rarely also incorporate genomic data produced by bioinformaticians.\nConversely, bioinformaticians often use different analytic tools to extract useful data from the genomes, such as presence/absence of markers of risk (like AMR and virulence genes, and genotyping information (such as MLST, cgMLST, and other species-specific typing), and phylogenetic relationships (trees), as well as quality metrics.\nThe genomic epidemiologist’s job often starts by combining data from diverse sources that might not be complete or standardised to facilitate the identification of relevant patterns for meaningful interpretation and intervention. The data and any patterns often need to be digested and summarised for decision makers.\nThe Centre for Genomic Pathogen Surveillance develops free web applications for integration and visualisation of surveillance (and other) data, called data-flo and Microreact. The aim of this module is to highlight the role of the various analytics and sources of data in pathogen surveillance, and how streamlined data integration is essential for real-time decision making, while also introducing resources that can facilitate the inclusion of this topic in teaching curricula."
  },
  {
    "objectID": "module4/module4.html#input-files",
    "href": "module4/module4.html#input-files",
    "title": "Module 4",
    "section": "Input files:",
    "text": "Input files:\nhttps://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa?usp=sharing"
  },
  {
    "objectID": "module4/module4.html#resources-needed",
    "href": "module4/module4.html#resources-needed",
    "title": "Module 4",
    "section": "Resources needed",
    "text": "Resources needed\nInternet access, Projector & screen, Participants should bring their laptops equipped with Google Chrome or Mozilla Firefox.\nAccess to: https://microreact.org/, https://pathogen.watch/, https://data-flo.io/"
  },
  {
    "objectID": "module4/module4.html#introduction",
    "href": "module4/module4.html#introduction",
    "title": "Module 4",
    "section": "Introduction",
    "text": "Introduction\nNote: This case-study is based on real sequence data from the article “Molecular epidemiology analysis of SARS-CoV-2 strains circulating in Romania during the first months of the pandemic.” by Surleac, Marius, et al., https://doi.org/10.3390/life10080152 , but it has been modified for the purposes of teaching this course. \n\n\n\nThe workflow of this exercise\n\n\nExplore the epidemiological information related to 24 SARS-CoV-2 samples from Romania.\n\n\n\nCountry\nTown\nNAME\nDate\n\n\n\n\nRomania\nBucuresti\nEPI_ISL_468134\n21/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468135\n22/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468136\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468137\n22/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468138\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468139\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468140\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468141\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468142\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468143\n25/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468144\n25/03/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468145\n28/03/2020\n\n\nRomania\nSuceava\nEPI_ISL_468146\n08/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468147\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468148\n11/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468149\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468150\n12/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468151\n15/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468152\n16/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468153\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468154\n17/04/2020\n\n\nRomania\nBucuresti\nEPI_ISL_468155\n18/04/2020\n\n\nRomania\nSuceava\nEPI_ISL_468157\n29/05/2020\n\n\nRomania\nSuceava\nEPI_ISL_468158\n29/05/2020\n\n\n\nThe genomes of the isolates presented in the table above have been sequenced, assembled and the assemblies have been uploaded to Pathogenwatch for further analyses.\nActivity 1. Data analysis of 24 SARS-CoV-2 genomes from Romania, collected during the early months of the pandemic.\nExplore a collection of 24 SARS-CoV-2 genomes in Pathogenwatch.\nhttps://pathogen.watch/collection/5hpqvjar7gfa-sars-cov-2-romania-v1\nQ1: In the statistics tab, check the genome lengths. Why do you think these have variable values?\nQ2: Why are the values of the N50 equal to the values of the genome lengths?\nQ3: What do you think of the QC statistics of the genome assemblies overall?\nQ4: Go to the Typing tab. What do you think is a “lineage” of SARS-CoV-2? Why were “lineages” defined early in the pandemic? What lineages do these genomes belong to?\nQ5: Go to the “Notable mutations” tab. Observe that all the genomes in this collection belong to the same lineage and they all share the same notable mutations. What does this tell you about the diversity of this collection?\nQ6: Click on one of the genomes and see the “Genome card”. Check the details of this lineage in a global context, by pressing View B.1.1 in the COG-UK Global Microreact. Where did this lineage spread and when?\n\n\n\nPathogenwatch screenshot showing a genome card for a SARS-CoV-2 genome from the lineage B1.1 . Notice the View B.1.1 in the COG-UK Global Microreact link\n\n\n\n\n\nMicroreact view showing isolates from the B1.1 lineage.\n\n\nActivity 2. Data integration\nIn this activity, you will merge the epidemiological information related to the isolates, the bioinformatics analysis files produced by Pathogenwatch and with the tree in Newick format, to produce a Microreact.\nDownload all the files used in this exercise from https://drive.google.com/drive/folders/1yP-ectAQ0plj8G1rcd_UNqPOyFIC7axa\nGo to Data-flo: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nClick on the Run button. Load the input files. Notice that you need an “access token” from Microreact.\n\n\n\nThe RUN button in Data-flo\n\n\n\n\n\nYou will find your access token in your Microreact account, under “Account settings”\n\n\n\n\n\nThe output of data-flo is a link to a microreact\n\n\n\n\n\nThe microreact produced by the data-flo\n\n\nCopy the data-flo to your account.\n\n\n\nOnce you are logged in to your data-flo account, you will be able to copy a public data-flo to your own account. Press on the symbol showing two overlapping papers on the right top corner to copy the current data-flo to your account\n\n\nRun the data-flo in debug mode and explore how data is being transformed between steps.\n\n\n\nPress the bug symbol at the top left corner to trigger the Debug mode. Notice the Data-flo debugger loading at the bottom of the screen.\n\n\nQ1: Which adaptors are responsible for data entry?\nQ2: What does the “Forward geocoding” adaptor do?\nQ3: Check the Microreact link produced by the data-flo.\nActivity 3. Data visualisation with Microreact\nExplore the Microreact produced by the Data-flo in the previous step.\nColour the leaves of the tree by “Town” and display the metadata columns for “Notable mutations” and the “Pangolin lineage”.\n\n\n\nIn order to set the colour of the leaves of a tree in Microreact, go to the eye symbol on top right and choose “Town” in the “Colour Column”\n\n\n\n\n\nSelect which columns you would like to display in the metadata blocks\n\n\nYour personal Microreact should like this one: https://microreact.org/project/sars-cov-2-romania1\nQ1: Look at the timelime. Which is the first isolate sampled from this collection? When was it sampled and where?\nQ2: When was the first isolate from the Bucharest sampled and when was the first isolate from Suceava sampled?\nActivity 4. Data analysis with context genomes\nThe bioinformatician in your team has found another 26 SARS-CoV-2 genomes from Romania in a public database and wants to include them in your study.\nView the whole collection of 51 genomes in Pathogenwatch https://pathogen.watch/collection/gqhf7ndghsim-test2\nSelect one of the genomes that belong to the lineage B.1.408 and open the “Genome card”.\nThen click on the View B.1.408 in the COG-UK Global Microreact\nQ1: How many entries of this sublineage are found in the database?\nQ2: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.408 shown in Microreact\n\n\nSelect the genome that belongs to the B.1.520 lineage, open its “Genome card” and click on the View B.1.520 in the COG-UK Global Microreact.\nQ3: In what countries was this sub-lineage found ? What does this tell you about the transmission links between Romania and those countries?\n\n\n\nLineage B.1.520 shown in Microreact\n\n\nActivity 5. Data integration for the collection of 51 genomes.\nLoad the same Data-flo as in Activity 2: https://data-flo.io/run?4LxncBtNr69TmfzTGj9Szm\nRun the data-flo with the new set of input files and load the new Microreact.\nActivity 6. Explore the new Microreact.\nColour the nodes of the tree by “Town” and display the mutations in the metadata blocks.\nYour microreact should look like this: https://microreact.org/project/sars-cov-2-romania-2\nQ1: According to this study, where did SARS-CoV-2 spread first outside of Suceava and Bucuresti?\nQ2: Where was the isolate belonging to the B.1.520 lineage found in Romania?\nActivity 7. Discuss the Romanian sequences in a global context.\n\n\n\nFigure from the original study published at https://doi.org/10.3390/life10080152\n\n\nQ1: How many introductions of SARS-CoV-2 into Romania can you identify based on this figure?\nQ2: Why do you think that the isolates from Suceava are interleaved with isolates from abroad?"
  },
  {
    "objectID": "module4/module4.html#appendix",
    "href": "module4/module4.html#appendix",
    "title": "Module 4",
    "section": "Appendix",
    "text": "Appendix\nData-flo Documentation: https://docs.data-flo.io/introduction/readme\nIf you found data-flo useful and would like to continue using it, start by creating an account https://data-flo.io/signin. There are a few public workflows with useful transformations such as left join two CSVs (https://data-flo.io/run?bzYR3DtxRJsBty5ezb5LoJ) or the Geocoder (https://data-flo.io/run?kvpsi3T8V) that you can try.\n\nA similar workflow to the one used on this course can be found in the documentation https://docs.data-flo.io/tutorials/prep-outbreak-data-for-microreact. In this workflow, one of the inputs is a SNP matrix. \nData-flo also allows you to import data from a Google Spreadsheet (https://docs.data-flo.io/using-data-flo/specific-adaptors/google-spreadsheet) or from a database (https://docs.data-flo.io/tutorials/common-use-cases-solved/connect-directly-to-a-database) (as opposed to downloading data from the spreadsheet/database into a file). \n\nMicroreact documentation: https://docs.microreact.org/\nTo be able to edit and manage projects, please create your own Microreact Account https://microreact.org/api/auth/signin. \nYou can customise the colours of your visualisations by assigning colours to the different variables before you create the Microreact project and/or after the project was created https://docs.microreact.org/instructions/labels-colours-and-shapes.\nEveryone with access to a Microreact project has access to the data the project uses. Privacy and permissions can be configured to change who can access the project and its data https://docs.microreact.org/instructions/access-control-and-project-sharing."
  },
  {
    "objectID": "module3/module3.html",
    "href": "module3/module3.html",
    "title": "Module 3",
    "section": "",
    "text": "Bacterial genome assembly data analysis\n\nPart 1: Antimicrobial resistance identification using ARIBA\nThe first part of this exercise is broadly based on a tutorial by Martin Hunt.\n\n\nIntroduction\nARIBA is a tool that identifies antibiotic resistance genes. This tutorial will walk you through the analysis of the Staphylococcus aureus data set used in the paper:\nNovel multidrug-resistant sublineages of Staphylococcus aureus clonal complex 22 discovered in India\nAbrudan, Shamanna et al, mSphere 2023 https://doi.org/10.1128/msphere.00185-23\nLearning outcomes By the end of this tutorial you can expect to be able to:\n• Download and prepare the standard AMR databases for use with ARIBA\n• Prepare your own database for use with ARIBA\n• Perform QC on input data and understand why QC is important\n• Run ARIBA on several samples to identify antibiotic resistance\n• Understand the different flags produced by ARIBA\n• Summarise ARIBA results for several samples\n• Query the AMR results produced by ARIBA\n• Use Phandango to visualise ARIBA results\nThis module comprises the following sections:\n1. Run ARIBA on a reference database\n2. View summarized results using Phandango\n\nYou can run the commands in this module in the Amazon EC2 instance you were provided by your course instructor.\nARIBA should be already installed on this machine. If you need to use a different machine, please follow the instructions from Module 1 of this course.\n1. Run ARIBA on a reference database\nConfigure the Resfinder and CARD reference databases.\nariba getref resfinder out.resfinder\nariba prepareref -f out.resfinder.fa -m out.resfinder.tsv out.resfinder.prepareref\nariba getref card out.card\nariba prepareref -f out.card.fa -m out.card.tsv out.card.prepareref\nHow to run on one sample\nARIBA needs the database directories, for eg, out.resfinder.prepareref or out.card.prepareref, and two sequencing reads files reads.1.fastq.gz and reads.2.fastq.gz. The command to run ARIBA is:\nariba run out.resfinder.prepareref reads.1.fastq.gz reads.2.fastq.gz outdir\nThe above command will make a new directory called outdir that contains the results.\nRun ARIBA on all samples\nThe S. aureus dataset consists of many samples, and we need to run ARIBA on each sample, which can be done with a “for” loop. We assume that the reads files are named like this:\nname1_1.fastq.gz name1_2.fastq.gz\nname2_1.fastq.gz name2_2.fastq.gz\nname3_1.fastq.gz name3_2.fastq.gz\nThen we can run ARIBA on all samples like this (you may need to edit this command depending on how your own files are named):\nEg:\nfor sample in `ls *.1.fastq.gz | sed 's/\\.1.fastq.gz//'`\ndo\n  ariba run out.resfinder.prepareref $sample_1.fastq.gz $sample_2.fastq.gz $sample.ariba\ndone\nThe output directory of each sample is called $sample.ariba, for example name1.ariba is the output directory for sample name1.\nEXERCISE: Using the information above, write a script to fit the file names and paths in your own context and run ARIBA on all samples.\nARIBA output\nThe format of the output files are described here.\nViewing ARIBA results in Phandango\nThis section describes how to use Phandango to view a summary of ARIBA results from many samples.\nThe most important output file from ARIBA is the report called report.tsv. For this tutorial, we have all sample reports in the directory /home/ubuntu/Data/ARIBA_output\nls /home/ubuntu/Data/ARIBA_output | wc -l\nARIBA has a functon called “summary” that can summarise presence/absence of sequences and/or SNPs across samples. It takes at least two ariba reports as input, and makes a CSV file that can be opened in your favourite spreadsheet program, and also makes input files for Phandango. The two Phandango files (a tree and a CSV file) can be dropped straight into the Phandango page for viewing.\nThe tree that ARIBA makes is based on the CSV file, which contains results of presence/absence of sequence and SNPs, and other information such as percent identity between contigs and reference sequences. This means that it does not necessarily represent the real phylogeny of the samples. It is more accurate to provide a tree built from the sequencing data. For this reason, we will use a pre-computed tree file /home/ubuntu/Data/tree_for_phandango.tre.\nBasic usage of ariba summary\nCheck the options of ariba summaryusing the following command:\nariba summary -h\nFirst, let’s run ariba summary using the default settings, except we will skip making the tree:\nariba summary --no_tree out /home/ubuntu/Data/ARIBA_output/*.tsv\nWe can see that this made two files:\nls out.*\nThey are the same except for the first line, which has Phandango-specific information. ARIBA uses the filenames as sample names in the output:\nhead -n 2 out.phandango.csv\nThe first name is “/home/ubuntu/Data/ARIBA_output/*/sample.tsv”, and the rest are named similarly.\nThis is not ideal, as it will look ugly in Phandango. Further, the names must exactly match the names in the tree file for Phandango to work (have a look in the tree /home/ubuntu/Data/trees/tree_for_phandango.nwk). You could do a little hacking here using the Unix command sed on the CSV file. Instead, we can supply ARIBA with a file of filenames that also tells ariba what to call the samples in its output CSV files. Instead of “/home/ubuntu/Data/ARIBA_output/sample.tsv”, we would like to simply use “sample”, which is cleaner and matches the tree file. It also means we can (and will) repeatedly run ariba summary with different options, and get output files that can be loaded straight into Phandango. This is one way to make the file with the naming information:\nls /home/ubuntu/Data/ARIBA_output/*/report.tsv | awk -F/ '{name=$(NF-1); sub(/\\.ariba$/, \"\", name); print $0, name}' &gt; ARIBA_output/filenames.fofn\nThe file is quite simple. Column 1 is the filename, and column 2 is the name we would like to use in the output.\n head ARIBA_output/filenames.fofn\nNow we can rerun summary using this input file. Note the use of the new option –fofn.\nariba summary --no_tree --fofn ARIBA_output/filenames.fofn ARIBA_output/out /home/ubuntu/Data/ARIBA_output/*/report.tsv\nCheck that the renaming worked:\nhead -n 2 out.phandango.csv\nNow go to Phandango and drag and drop the files out.phandango.csv and trees/tree_for_phandango.nwk into the window. The result should like this\n\nThis a very high-level summary of the data. For each cluster, it is simply saying whether or not each sample has a ‘match’. Green means a match, and pink means not a match. For presence/absence genes, this means that the gene must simply be there to count as a match. If it is a “variant only” gene, then the gene must be there and one of the variants that we told ARIBA about earlier when generating the ARIBA database.\nMore information per cluster\nIn addition to a simple “yes” or “no” as to whether a sample “matches” a given cluster (as explained above), more columns can be output for each cluster. See the ARIBA summary wiki page for a full description of the options.\nVariants In the previous screenshot, where the option –preset cluster_all, there are two variant columns: “known_var” and “novel_var”. Green means “yes” and pink means “no”.\nPart 2: Antimicrobial Resistance Identification using Pathogenwatch\nBrowse the public collection:\nhttps://pathogen.watch/collection/mxebr8oz0wjm-module2-s-aureus\nDownload the AMR genes and AMR SNPs.\nPart 3: Compare results obtained with ARIBA Resfinder versus Pathogenwatch\nOpen the ARIBA summary files and compare them with the AMR genes and AMR SNPs files you downloaded from Pathogenwatch.\n\nWhich tool produced more results? Why?\nWhich tool would you use to report AMR in your collection of S. aureus?"
  },
  {
    "objectID": "module5/module5.html",
    "href": "module5/module5.html",
    "title": "Module 5",
    "section": "",
    "text": "How to setup your Amazon EC machine\n\nOnly attempt this if you have a back cad available.\nCreate an Amazon account\nCreate an instance\nSetup your bioinformatics tools\n\n\n\nOnce you are happy with your result, create an image, which you can use later.\nhttps://medium.com/@virajpatoliya/easy-guide-how-to-clone-restore-aws-ec2-instances-3d29e156ac65"
  },
  {
    "objectID": "microreact/microreact.html",
    "href": "microreact/microreact.html",
    "title": "Microreact tutorial",
    "section": "",
    "text": "For Microreact documentation, go to https://docs.microreact.org/"
  },
  {
    "objectID": "microreact/microreact.html#task-1-create-an-editable-project.",
    "href": "microreact/microreact.html#task-1-create-an-editable-project.",
    "title": "Microreact tutorial",
    "section": "Task 1: Create an editable project.",
    "text": "Task 1: Create an editable project.\n\n\n\nTask 1: Create an editable project. Step 1: Notice the crossed out pen on right top corner of your screen. This indicates that you cannot edit the current Microreact project.\n\n\n\n\n\nTask 1: Create an editable project. Step 2: Click the crossed out “Pen” symbol in the top right of the screen. A window appears asking you to “SIGN IN TO EDIT”.\n\n\n\n\n\nTask 1: Create an editable project. Step 3: Once you sign in, the message changes, and you are invited to “MAKE A COPY” of the current project. Make a copy, to proceed.\n\n\n\n\n\nTask 1: Create an editable project. Step 4: Notice that, once you made a copy of the project, the crossed out “pen” symbol will change to a “normal pen”, and you will be able to edit and save the project."
  },
  {
    "objectID": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "href": "microreact/microreact.html#task-2-present-whole-genome-sequencing-quality-control-wgs-qc-statistics-in-a-chart.",
    "title": "Microreact tutorial",
    "section": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.",
    "text": "Task 2: Present whole genome sequencing quality control (WGS QC) statistics in a chart.\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 1: Select the “Kpn Colombia” view. Click on the “Pen” symbol on the top right menu. Click on the “Create New Chart”\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 2: Drag the new chart to overlap with the tree.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 3: An empty panel for the new chart will be shown on top of the tree panel.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 4: In the Chart Type dropdown list select “Bar Chart”.\n\n\n\n\n\nTask2: Present WGS QC statistics in a chart. Step 5: A new window appears. In the X Axis Column, select “WGS_QC_no_contigs” and for “Maximum number of bins” select 10.\n\n\n\n\n\nTask 2: Present WGS QC statistics in a chart. Step 6: The bar chart will look like above. Observe that most genomes have less the 100 contigs."
  },
  {
    "objectID": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "href": "microreact/microreact.html#task-3-what-are-the-dominating-sequence-types-sts-in-colombia",
    "title": "Microreact tutorial",
    "section": "Task 3: What are the dominating sequence types (STs) in Colombia?",
    "text": "Task 3: What are the dominating sequence types (STs) in Colombia?\n\n\n\nTask 3: What are the dominating STs in Colombia? Now that you’ve created one chart, you can create another one! Step 1: Go to the “Pen: symbol on the right hand side and click on the”Create New Chart”.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 2: The new chart can stay right on top of the previously created one.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 3: Notice a white canvas on top of the previously generated chart.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 4: Once again, from the Chart Type dropdown menu, select “Bar Chart”, and when the new view shows up on the “X Axis Column”, select ST.\n\n\n\n\n\nTask 3: What are the dominating STs in Colombia? Step 5: A new chart will appear. The labels on the x-axis appear squished and they are hard to read. Drag the panel divider on the left hand side of the chart, to increase the width of the panel.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 6: The information on the x-axis should be readable now. The 3 most abundant STs are ST11, ST258 and ST512.\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 7: Click the “Views” panel on the left hand side, hover over “Kpn Colombia”, click on the three dots on the corner of the view and hit “Update View”\n\n\n\n\n\nTask 3: What are the dominant STs in Colombia? Step 8: Go to the Save icon on the right corner, press the icon and choose “Update This Project”"
  },
  {
    "objectID": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "href": "microreact/microreact.html#task-4-plot-metadata-blocks-for-the-carbapenamase-genes-ctx-m-15-ndm-1-kpc-and-oxa.-what-are-the-prevalent-amr-mechanisms-detected",
    "title": "Microreact tutorial",
    "section": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?",
    "text": "Task 4: Plot metadata blocks for the carbapenamase genes CTX-M-15, NDM-1, KPC and OXA. What are the prevalent AMR mechanisms detected ?\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 1: Press the icon on the top right of the tree panel.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 2: In the Metadata blocks dropdown list, tick all boxes containing KPC, NDM, VIM, OXA and CTX-M-15 genes.\n\n\n\n\n\nTask 4: Plot metadata blocks for CTX-M-15, NDM-1, KPC and OXA genes. What are the prevalent AMR mechanisms detected ? Step 3: The tree panel will show metadata columns. Yellow indicates presence of a certain gene and green indicates absence. In the panel shown here, it appears that the most common genes present are KPC-2, KPC-3, NDM-1 and CTX-M-15."
  },
  {
    "objectID": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "href": "microreact/microreact.html#task-5-which-sts-are-associated-with-the-presence-of-carbapenamase-genes",
    "title": "Microreact tutorial",
    "section": "Task 5: Which STs are associated with the presence of carbapenamase genes?",
    "text": "Task 5: Which STs are associated with the presence of carbapenamase genes?\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 1: Go to the “Metadata blocks” and check the ST box.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? Step 2: Observe the new metadata column next to the tree, with the header “ST”.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? In the tree with added metadata blocks, we can observe a large brown block of isolates belonging to ST258. You will need to check the Legend on the very right of Microreact.\n\n\n\n\n\nTask 5: Which STs are associated with the presence of carbapenamase genes? On a close look, we can observe that ST258 is associated with the presence of the carbapenamase gene KPC-3."
  },
  {
    "objectID": "data-flo/data-flo.html",
    "href": "data-flo/data-flo.html",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n“Steps” in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through “Google spreadsheet” adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a “Join datatables” adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the “Update Microreact project” adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is “Lab to bioinformatics”\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select “NEW DATAFLOW”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on “Spreadsheet file” on the list of adaptors. Hover over the “i” button next to “Spreadsheet file” to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on “Spreadsheet file” on the list of adaptors, will add an adaptor (which looks like a box), with the title “Spreadsheet file” on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under “BINDING TYPE”, such as “Bind to a Dataflow input” and “Bind to another transformation”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of “BINDING TYPEs”, Select “Bind to a Dataflow input” and press the “+” next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with “file” will appear on the canvas, linked to the “Spreadsheet file” adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little “bug icon” on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the “bug icon” will trigger the Dataflow Debugger and you will be shown a “Choose file” button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the “Choose file” button and select a spreadsheet from your local computer. Click the “data” on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the “data” on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "data-flo/data-flo.html#a-short-introduction",
    "href": "data-flo/data-flo.html#a-short-introduction",
    "title": "Data-flo tutorial",
    "section": "",
    "text": "Data-flo (https://data-flo.io/) is a system for customised integration and manipulation of diverse data via a simple drag and drop interface.\nFor Data-flo documentation, go to https://docs.data-flo.io/introduction/readme\nData-flo can easily combine epidemiological data, genomic data, laboratory data, and various metadata from disparate sources (i.e., different data systems) and formats.\nData-flo provides a visual method to design a reusable pipeline to integrate, clean, and manipulate data in a multitude of ways, eliminating the need for continuous manual intervention (e.g., coding, formatting, spreadsheet formulas, manual copy-pasting).\nData-flo pipelines are combinations of ready-to-use data adaptors that can be tailored, modularised and shared for reuse and reproducibility. Once a Data-flo pipeline has been created, it can be run anytime, by anyone with access, to enable push-button data extraction and transformation. This saves significant time by removing the bulk of the manual repetitive workflows that require multiple sequential or tedious steps, enabling practitioners to focus on analysis and interpretation.\n\n\n“Steps” in Data-flo are called ADAPTORS. There are three main types of adaptors, which serve different functions.\n\nImporting data\nManipulating & transforming data\nExporting data\n\n\n\n\nData-flo editing view. Examples of Data-flo adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo that imports data from Google spreadsheets, through “Google spreadsheet” adaptors.\n\n\n\n\n\nData-flo editing view. This is an example of a Data-flo where two datatables are merged using a “Join datatables” adaptor.\n\n\n\n\n\nData-flo editing view. Using various adaptors, columns in a datatable can be removed or renamed, specific strings or blank values can be replaced, dates can be reformatted, distinct lists can be generated.\n\n\n\n\n\nData-flo editing view. In this example, a Data-flo pushes updated data to a Microreact project and supplies the URL for that project using the “Update Microreact project” adaptor.\n\n\n\n\n\n\n\n\nHow to run a Data-flo, Step 1: Go to Data-flo transformations: data-flo.io/transformations/dataflows\n\n\n\n\n\nHow to run a Data-flo, Step 2: Select your favourite Data-flo. The Data-flo shown in this example is “Lab to bioinformatics”\n\n\n\n\n\nHow to run a Data-flo, Step 3: Click on the RUN tab to get to the RUN page.\n\n\n\n\n\nHow to run a Data-flo, Step 4: Hit the RUN button.\n\n\n\n\n\nHow to run a Data-flo, Step 5: Check out the output. In this example, the output is a Microreact link. You can choose to RUN AGAIN the Data-flo, and this will update your results (in case the input had been changed, of course).\n\n\n\n\n\nCreate a Data-flo from scratch, Step 1: Go to the + sign on the bottom right of your screen.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 2: Select “NEW DATAFLOW”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 3: Select your preferred ACCESS CONTROL.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 4: Add adaptors to your Data-flo. On the left-hand side of Data-flo, you can find the list of available adaptors.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 5: Try to retrieve data from a spreadsheet. Click on “Spreadsheet file” on the list of adaptors. Hover over the “i” button next to “Spreadsheet file” to see basic information about this adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 6: Clicking on “Spreadsheet file” on the list of adaptors, will add an adaptor (which looks like a box), with the title “Spreadsheet file” on your canvas. When you click on *file on the left side of the adaptor, the view on the right hand side of your screen will change, and you will be able to see a list of options under “BINDING TYPE”, such as “Bind to a Dataflow input” and “Bind to another transformation”.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 7: From the list of “BINDING TYPEs”, Select “Bind to a Dataflow input” and press the “+” next to INPUT ARGUMENT.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 8: A box with “file” will appear on the canvas, linked to the “Spreadsheet file” adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 9: To run a Data-flo, press the little “bug icon” on top, on the right hand side of the Save button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 10: Pressing the “bug icon” will trigger the Dataflow Debugger and you will be shown a “Choose file” button.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 11: Press the “Choose file” button and select a spreadsheet from your local computer. Click the “data” on the right hand side of the adaptor.\n\n\n\n\n\nCreate a Data-flo from scratch, Step 12: Clicking on the “data” on the right hand side of the adaptor, will retrieve a view of the file selected. In this case, the Dataflow Debugger is showing the first 3 rows of a file containing epidemiological data.\n\n\n\n\n\n\n\n\nHow to share your Data-flo, Step 1: Press the downward facing arrow shown here on the top right corner. This will trigger the export of a .json file, which can be sent and shared.\n\n\n\n\n\nHow to share your Data-flo, Step 2: If you have received a .json Data-flo file, press the + sign at the bottom right of your screen\n\n\n\n\n\nHow to share your Data-flo, Step 3: Press the IMPORT button and then select your .json file containing the Data-flo."
  },
  {
    "objectID": "agenda.html",
    "href": "agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "Prerequisites for following this course.\n\nYou are expected to be comfortable with bash and also with a text editor such as vi or nano. If you are not, try to practice a few commands before you attempt to follow the steps below.\nPlease make sure you have received from your course instructor the Public IPv4 of your Amazon EC2 instance, the name of the machine you are going to work on and also a public key to access the machine via ssh."
  },
  {
    "objectID": "Syllabus.html",
    "href": "Syllabus.html",
    "title": "Training Syllabus",
    "section": "",
    "text": "Topics to be addressed during the workshop\n\n\n\nTraining Objectives\nLearning objectives. By the end of the workshop, the participants will be able to\n\n\nTargeted competencies and Knowledge, Skills and Attitudes\n\n\nTraining Methods and Instructional Strategies\n\n\nDuration of training\n\n\nSchedule of sessions\n\n\nAssessment and Evaluation\n\n\nResources\n\n\nRecommended background knowledge"
  },
  {
    "objectID": "module1/module1.html",
    "href": "module1/module1.html",
    "title": "Module 1",
    "section": "",
    "text": "Module 1: Set up your bioinformatics working environment\n\n\nPart 1: Connect to your Amazon EC2 instance via ssh\nBefore you start, make sure you received the IPv4 address of the virtual machine and your private key to connect to the machine from your course coordinator.\nThe virtual machines provided during this course are intended exclusively for use with the course material.\n\nUsing a Mac\nOpen your Terminal application on your local machine.\nThen type\nchmod 400 /local/path/to/student.pem\nssh -i /local/path/to/key.pem  ubuntu@xxx.xxx.xxx.xxx\nReplace the “xxx.xxx.xxx.xxx” with the Public IPv4 of your Amazon instance and the /local/path/to/student.pem with the local path to your student.pem file that was communicated to you previously.\nFor more information on how to connect to your virtual machine, access https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-ssh.html .\n\n\nUsing a Windows machine\nSee https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/connect-linux-inst-from-windows.html\n\n\nHow to connect to your Amazon EC2 instance via your browser (during the course)\nType the Public IPv4 in your browser address bar, eg: https://xxx.xxx.xxx.xxx/\nType in the username ubuntu, and the password provided to you by the course instructor.\nOnce you log in, you should see the welcome screen.\nPlease reject any invitation to update the system.\n\n\nHow to modify your $PATH variable\nOnce you are connected to your Amazon EC2 Ubuntu instance, try to view and edit your configuration files. \nCheck the configuration files:\nls -a ~/.\nEdit the .profile file and add the paths in the PATH variable\nEg:\nvi ~/.profile\nPATH=\"/home/ubuntu/Software/mash-Linux64-v2.3:$PATH\"\n\n\n\nPart 2: Configure your Amazon EC2 Ubuntu instance\nYour Amazon EC2 Ubuntu instance should be ready to go right away. However, in order to go through the next modules of this course, you will need to install the list of tools below.\nFirst, familiarise yourselves with the tools. Understand what is their role in an NGS pipeline, what is the required input and the expected output. Who developed them and when? Do they have dependencies?\nFor each tool that you attempt to install, write down the steps you took to achieve that in the Shared student observations file, which you have received from your course coordinator.\n\nFastQC\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nBactinspector\nhttps://gitlab.com/antunderwood/bactinspector\n\n\nVelvet assembler\nhttps://github.com/dzerbino/velvet\n\n\n\nACT and Artemis\nhttps://www.sanger.ac.uk/tool/artemis-comparison-tool-act/\n\n\n\nUnicycler \nhttps://github.com/rrwick/Unicycler?tab=readme-ov-file#build-and-run-without-installation\n\n\n\nSPADES\nhttps://github.com/ablab/spades\n\n\n\nQuast\nhttps://github.com/ablab/quast\n\n\n\nbrew\nhttps://docs.brew.sh/Homebrew-on-Linux\n\n\n\nmakeblastdb and tblastn\nhttps://www.ncbi.nlm.nih.gov/books/NBK569861/\nprokka\nhttps://github.com/tseemann/prokka\n\n\n\nresfinder\nhttps://github.com/cadms/resfinder\n\nARIBA\nhttps://sanger-pathogens.github.io/ariba/"
  }
]